{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy, Compression, and Huffman Variable Length Encoding\n",
    "stough 202-\n",
    "\n",
    "1. [Computing Entropy](#entropy)\n",
    "1. [Huffman Trees](#huffman)\n",
    "1. [Variable-length encoding and decoding](#encode-decode)\n",
    "    - [Decoding](#decode-exercise)\n",
    "    - [Reconstructing](#reconstruct-exercise)\n",
    "    - [In Color](#color-exercise)\n",
    "\n",
    "Images and videos can be quite large. At 60 frames per second, a 4K Ultra HD movie of 2 hours could require \n",
    "\n",
    "\\begin{equation*}\n",
    "((3840\\cdot2160)pixels \\cdot 3\\frac{byte}{pixel} = 24883200 \\frac{byte}{frame}\\\\\n",
    "\\frac{24883200 \\frac{byte}{frame}\\cdot60\\frac{frame}{second}\\cdot60\\frac{second}{minute}\\cdot120 minutes}{10^9\\frac{byte}{GB}} = 10749.5 GB,\n",
    "\\end{equation*}\n",
    "\n",
    "or about 163 blu-ray discs to store without compression. So clearly compression is a thing. \n",
    "\n",
    "Compression takes advantage of different kinds of **redundancy** in the signal. Images in particular have a lot of redundancy. **Spatial and Temporal redundancy** are characterized by the fact that most pixels are closely related their neighbors. This is due to the fact that any images of human interest are likely to have objects comprised of regions of similar color. *Temporal* redundancy is the video case, where a pixel over time changes little usually. \n",
    "\n",
    "**Coding redundancy** is the one we're dealing with here. Most images we consider are 3-channel images with 8 bits per pixel per channel. We need 8 bits to represent a value in $[0, 255]$. Variable-length encoding can take advantage of the differential frequency of certain pixel values over others, encoding more common pixels with fewer bits, and paying that off by using more bits to represent less common pixels.\n",
    "\n",
    "- **Compressibility**: The degree to which the information in an image or video can be represented using fewer bits. In the example above of the highly compressible 4K Ultra HD, we might say the *compression ratio* is ~160 (display size over storage size). \n",
    "- **Entropy**: a measure of the information content in a signal. \n",
    "    - signal: here, think of this as seeing each individual pixel in the image in order.\n",
    "    - Think of entropy as the surprise, or \"unpredictability\" of the signal. Each new pixel value $k$ in the signal gives you some information,\n",
    "    \\begin{equation*}\n",
    "    I(k) = -\\log_2(p(k)),\n",
    "    \\end{equation*}\n",
    "    where $p(k)$ is the probability of observing $k$ in the signal. For example, if the probability of observing $k$ is 1, as in 100\\% of all the pixels have the value $k$, then there is no information/surpise, or 0 bits, associated with observing it. If $p(k) = \\frac{1}{2}$, then that's 1 bit of information. (We won't consider the symbols $\\kappa$ that *never* show up, since observing such a symbol would be infinitely surprising :-). \n",
    "    - For our images in the range $[0, 255]$, we can measure the entropy or information content of the image as \n",
    "    \\begin{equation*}\n",
    "    \\tilde{H} = -\\sum_{k=0}^{255} p(k)\\log_{2}p(k)\n",
    "    \\end{equation*}\n",
    "    This is just an average of how many bits of information each newly observed pixel gives us, weighted by how likely that pixel value is. $p(k)$ is just the normalized histogram values.\n",
    "    \n",
    "In this notebook we'll explore Huffman variable-length encoding its effectiveness in some images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Notice I've added our own custom Huffman utilities `huff_utils` and [`scipy.stats.entropy`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html). Study `build_huff_tree` in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "import skimage.color as color\n",
    "from ipywidgets import VBox, HBox, FloatLogSlider\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# For importing from alternative directory sources\n",
    "import sys  \n",
    "sys.path.insert(0, '../dip_utils')\n",
    "\n",
    "\n",
    "from huff_utils import (build_huff_tree,\n",
    "                        build_huff_encoder,\n",
    "                        build_huff_pair,\n",
    "                        load_huffable_image,\n",
    "                        test_tree_making)\n",
    "from matrix_utils import (arr_info,\n",
    "                          make_linmap)\n",
    "from vis_utils import (vis_rgb_cube,\n",
    "                       vis_hsv_cube,\n",
    "                       vis_hists,\n",
    "                       vis_pair,\n",
    "                       lab_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='entropy'></a>\n",
    "\n",
    "## Computing Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = plt.imread('../dip_pics/happy128.png')\n",
    "Ih = load_huffable_image(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ih = load_huffable_image('../dip_pics/happy128.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_info(Ih)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(Ih, cmap='gray', interpolation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_hists(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many bits would we need?\n",
    "8*np.prod(Ih.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(257)\n",
    "freq, bins = np.histogram(Ih.ravel(), bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the average actual information per pixel in the image?\n",
    "entropy(freq, base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='huffman'></a>\n",
    "\n",
    "## Huffman Trees\n",
    "A Huffman tree is a binary tree structure whose leaf nodes represent the symbols in a signal (the pixel values in an image). The path from the root of the tree to a leaf node represents the bit encoding of the associated symbol (0 for left child, 1 for right child, for example). Let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree_making()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we start with pixel values in ['A', 'B', 'C', 'D']. The Huffman Tree construction algorithm basically sorts these from least to most frequent (a heap is useful for this) as a list of leaf nodes. \n",
    "\n",
    "- We then take the two least frequent nodes and combine them into one Huffman tree with the two pixel values as leaf nodes and the frequency associated with this tree as the sum of the two individual frequencies. \n",
    "- Then insert this tree back into the list. This is where a heap comes in useful, as a self-maintaining structure always providing access to the least frequent node. \n",
    "- Repeat. Each iteration we end up with one fewer in the list, until there is only one tree left. \n",
    "\n",
    "The structure of that final tree represents an optimal encoding given the relative frequencies provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='encode-decode'></a>\n",
    "\n",
    "## Variable-length encoding and decoding\n",
    "See the code in `huff_utils`. One thing you'll notice about the encoder is that a lot of pixel values require many more than even 8 bits!!! How could that possibly save space?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = build_huff_pair(Ih)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's encode the image.\n",
    "enI = ''.join([encoder[pix] for pix in Ih.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(enI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8*np.prod(Ih.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression ratio: old bits to new bits needed.\n",
    "8*np.prod(Ih.shape)/len(enI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bits per pixel we're actually using; compare to entropy determined above.\n",
    "len(enI)/np.prod(Ih.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='decode-exercise'></a>\n",
    "\n",
    "### Decoding \n",
    "\n",
    "So in the above we see how we can take an image--an array of pixel values--and encode it using the Huffman variable-length encoding. In short:\n",
    "```python\n",
    "I = load_huffable_image('../dip_pics/happy128.png')\n",
    "encoder, decoder = build_huff_pair(I)\n",
    "enI = ''.join(encoder[pix] for pix in I.ravel())\n",
    "```\n",
    "\n",
    "After the above section executes, `enI` is the encoded image (a sequence of bits), which can be stored or transmitted more efficiently. However, when display of the image is needed again, we're going to need to decode the bit sequence back into an array of pixel values. We can use the `decoder` dictionary to do this.\n",
    "\n",
    "Write a `decode_image` function that accepts such an encoded string (like `enI`) and the `decoder` dictionary, and outputs a list of intensites equal to uncompressed image `I.ravel()`. Show a validation test of this function. See [`array_equal`](https://numpy.org/doc/stable/reference/generated/numpy.array_equal.html): is the reconstructed array of intensities equal to the original (`I.ravel()` for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reconstruct-exercise'></a>\n",
    "\n",
    "### Reconstructing\n",
    "\n",
    "Now take the output of `decode_image` to reconstruct the original image `I`. Even though our encoded stream does not include the image shape, you may cheat there and use the original image shape “out of band” so to speak. In practice we could send along the image shape with the encoded stream without too much trouble. Show your reconstruction code and a figure supporting its success. See [`numpy.reshape`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='color-exercise'></a>\n",
    "\n",
    "### In Color\n",
    "Do this with a color image (that is not happyFace). Perform Huffman compression separately on each of the three channels and show that you can encode, and then decode and reconstruct such a color image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
