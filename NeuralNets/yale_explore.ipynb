{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Yale cropped faces dataset.\n",
    "Stough, DIP\n",
    "\n",
    "Here I'm exploring a part of the [Yale Extended Face Dataset B](http://vision.ucsd.edu/~iskwak/ExtYaleDatabase/ExtYaleB.html). This CroppedYale dataset consists of ~65 cropped face images for each of 38 individuals. The images are taken under different lighting conditions, but the face orientation stays the same. \n",
    "\n",
    "The dataset is organized in a very convenient way for PyTorch, as a root directory each of whose subdirectories represents a class and contains the images of that class. This is ideal for torch's [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder) Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# or widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import os\n",
    "\n",
    "\n",
    "# from keras.datasets import mnist\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch\n",
    "\n",
    "# For timing.\n",
    "import time\n",
    "tic, toc = (time.time, time.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Let's start by just getting the data, seeing what it is.\n",
    "As a Dataset, it's indexable with `__getitem__`. Indexing the Dataset shows it gives you a tuple. This is the input, target tuple that you can train for. Further, the `.classes` and `.class_to_idx` show the folder names corresponding to the target index in the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaleData = ImageFolder('/home/dip365/data/CroppedYale/')\n",
    "yaleData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaleData[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaleData.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Now, let's load the same dataset, BUT\n",
    "with a composition of transforms that gives us tensor data that our torch models can deal with. We'll turn it into grayscale, make sure all the images are the same size, and convert to Tensor, all through [built-in transforms](https://pytorch.org/docs/stable/torchvision/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaleData = ImageFolder('/home/dip365/data/CroppedYale/', \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Grayscale(),\n",
    "                           transforms.Resize((192,168), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                           transforms.ToTensor()\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.stack([yaleData[i][0] \n",
    "                       for i in np.random.choice(len(yaleData), 64)])\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(make_grid(samples, nrow=8, pad_value=1.0).permute(1,2,0))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## We can even do some [data augmentation](https://nanonets.com/blog/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2/) \n",
    "through the torch transforms available. Here we'll use [ColorJitter](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ColorJitter) to randomly perturb the brightness and contrast in an image each time we load it from our Dataset. Notice in the below I load the very same image 64 times, but it looks a little different each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaleData = ImageFolder('/home/dip365/data/CroppedYale/', \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Grayscale(),\n",
    "                           transforms.Resize((192,168), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                           transforms.ColorJitter(brightness=.5, contrast=.3), # Random recolorization every load.\n",
    "                           transforms.ToTensor()\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, let's view some random faces\n",
    "samples = torch.stack([yaleData[100][0] \n",
    "                       for i in range(64)])\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(make_grid(samples, nrow=8, pad_value=1.0).permute(1,2,0))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## We can also randomly split the data into \n",
    "as many subsets (like training and testing) as we like using torch's [random_split](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) utility. It doesn't support stratified splits though, so that we likely won't have equal proportions of the classes/subjects across splits.\n",
    "\n",
    "Here we'll do a 90%-10% split, then look at the distribution of classes/subjects in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_splits = np.round([f*len(yaleData) for f in [.9, .1]]).astype(np.int32)\n",
    "trainFaces, testFaces = torch.utils.data.random_split(yaleData, train_test_splits.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "testlabels = pandas.Series([yaleData.classes[tup[1]] for tup in testFaces])\n",
    "testlabels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Lastly, here I take a quick look at the Extended \n",
    "(multiple face orientations and lighting conditions) dataset. This is much larger, with \n",
    "some 585 images for each of the 38 subjects. Also each image is larger, at 480x640. \n",
    "The below assumes you've made the link (ln -s) connecting your local `data/ExtendedYaleB` to `~dip365/data/ExtendedYaleB/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extData = ImageFolder('/home/dip365/data/ExtendedYaleB/', # or I guess, '~dip379/data/ExtendedYaleB/'\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Grayscale(),\n",
    "                           transforms.Resize((480,640)),\n",
    "                           transforms.ToTensor()\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.stack([extData[i][0] \n",
    "                       for i in np.random.choice(len(extData), 64, replace=False)])\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(make_grid(samples, nrow=8, padding=4,pad_value=1.0).permute(1,2,0))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
