{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional NN on [MNIST](https://github.com/pytorch/examples/tree/master/mnist) with [torch](https://pytorch.org/)\n",
    "stough 202-\n",
    "\n",
    "Following a simple digit classification on MNIST, using a small convolutional neural network (CNN).\n",
    "\n",
    "A [convolutional layer](https://pytorch.org/docs/stable/nn.html#conv2d) solves for simple spatial filtering\n",
    "operations where the output (feature map) contains useful information for the downstream or deeper layers \n",
    "in the network. With $in\\_channels$ input channels, $out\\_channels$ output feature maps, and $kernel\\_size$ for the filtering, the layer will have $in\\_channels * out\\_channels * kernel\\_size^2 + out\\_channels$ parameters \n",
    "to optimize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# or widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import copy\n",
    "import tempfile\n",
    "\n",
    "# from keras.datasets import mnist\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Module\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# For timing.\n",
    "import time\n",
    "tic, toc = (time.time, time.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Network\n",
    "A more complicated network, using Convolutional Layers to transform the original image\n",
    "into a collection of features that a linear layer can use to discriminate \n",
    "among the classes/digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1) # padding is 0 by default, so \n",
    "                                            # we lose a pixel on each side.\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # input: 1x28x28, output: 32x26x26\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x) # input: 32x26x26, output: 64x24x24\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2) # input: 64x24x24, output: 64x12x12\n",
    "        x = self.dropout1(x) # randomly zero out some of the features. (in training only)\n",
    "        x = torch.flatten(x, 1) # flatten the 64x12x12 to a single dimension (9216) \n",
    "        x = self.fc1(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "#         output = torch.sigmoid(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test functions.\n",
    "During training, the optimizer modifies the parameters of the model in a way that minimizes the loss\n",
    "function. See more detail [here](https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step), but a lot is hidden from you. Just viewing it pythonically for example, it is not clear how the loss\n",
    "and the optimizer are connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    starttime = tic()\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    loss_avg = loss_sum/len(train_loader.dataset)\n",
    "    \n",
    "    endtime = toc()\n",
    "    print('\\nTrain set: Average loss: {:.4f} ({:.3f} sec)'.\\\n",
    "          format(loss_avg, \n",
    "                 endtime-starttime))\n",
    "    \n",
    "    return loss_avg\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            # test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = correct / len(test_loader.dataset)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * test_acc))\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Namespace to replace the argparse business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "# What is a Namespace? It looks like this. \n",
    "# Such objects allow you to use the dot operator, like a struct in C.\n",
    "# class Namespace:\n",
    "#     def __init__(self, **kwargs):\n",
    "#         self.__dict__.update(kwargs)\n",
    "\n",
    "args = Namespace(\n",
    "    no_cuda=False, \n",
    "    seed=1, \n",
    "    batch_size=64,\n",
    "    test_batch_size=1000,\n",
    "    epochs=5,\n",
    "    lr=1.0,\n",
    "    gamma=0.7,\n",
    "    log_interval=250,\n",
    "    save_model=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize the MNIST data\n",
    "We'll use the [torchvision transforms](https://pytorch.org/docs/stable/torchvision/transforms.html#) to \n",
    "modify the dataset without having to convert to numpy arrays ourselves. If organized correctly, we won't\n",
    "need our own Dataset class and collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you: https://www.aiworkbox.com/lessons/load-mnist-dataset-from-pytorch-torchvision\n",
    "# https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "mnist_trainset = datasets.MNIST(root='/home/dip365/data', train=True, download=True, \n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                ]))\n",
    "mnist_testset = datasets.MNIST(root='/home/dip365/data', train=False, download=True, \n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 4, 'pin_memory': False} if use_cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset,\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset,\n",
    "    batch_size=args.test_batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Instantiate the model and count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "# Count the number of parameters: \n",
    "print(f'model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Run the optimization\n",
    "- [Momentum, Learning Rate, etc](https://distill.pub/2017/momentum/)\n",
    "- [Learning Rate Schedulers](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train_loss.append(train(args, model, device, train_loader, optimizer, epoch))\n",
    "    results = test(args, model, device, test_loader)\n",
    "    test_loss.append(results[0])\n",
    "    test_acc.append(results[1])\n",
    "    scheduler.step()\n",
    "\n",
    "if args.save_model:\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://matplotlib.org/gallery/api/two_scales.html\n",
    "\n",
    "train_loss = np.array(train_loss)\n",
    "test_loss = np.array(test_loss)\n",
    "test_acc = np.array(test_acc)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(np.stack([train_loss, test_loss]).T);\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(test_acc, 'r--', label='test_acc');\n",
    "\n",
    "\n",
    "ax1.legend(labels=['train_loss', 'test_loss'], loc='upper left')\n",
    "ax2.legend(loc='upper right');\n",
    "\n",
    "# plt.savefig('../dip_outs/conv_MNIST_torch_training.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Let's get the output of the model for all the test data.\n",
    "Since the test_loader is also shuffled, we're going to want to keep track of the target too, to know what the answer should have been. In fact, let's just keep all of it. \n",
    "\n",
    "In the below cell we loop over all the test data, push it through the model and store the resulting classifier outputs. The expression `model(data.to(device))` sends the data to the GPU (where the model resides) and applies the model to it. The sequence that happens after, `.cpu().detach().numpy()`, takes the resulting outputs and brings it back to the cpu memory space in the form of a numpy array.\n",
    "\n",
    "A [notes on pretty-printing arrays](https://stackoverflow.com/questions/2891790/how-to-pretty-print-a-numpy-array-without-scientific-notation-and-with-given-pre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # make sure the model weights don't change.\n",
    "outputs = np.concatenate([model(data.to(device)).cpu().detach().numpy() \n",
    "              for data, target in test_loader], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last function in the model is a log_softmax, which is a log after softmax.\n",
    "# Softmax makes the outputs all in [0,1] and sum to 1, but then there is a log \n",
    "# after that. So we undo that to see the [0,1] numbers, which can be thought of \n",
    "# as probabilities.\n",
    "with np.printoptions(precision=4, suppress=True):\n",
    "    print(np.exp(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just for fun, let's see some of the mistakes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels = np.concatenate([target for data, target in test_loader], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_labels = np.argmax(outputs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(proposed_labels == correct_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_guesses = np.where(proposed_labels != correct_labels)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "which_wrong = np.random.choice(wrong_guesses, 64, replace=False)\n",
    "samples = torch.stack([mnist_testset[r][0]\n",
    "                       for r in which_wrong])\n",
    "plt.imshow(make_grid(samples, nrow=8, pad_value=1.0).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted: ' + ' '.join(['%d' % x for x in proposed_labels[which_wrong]]))\n",
    "print('Actual:    ' + ' '.join(['%d' % x for x in correct_labels[which_wrong]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join([str((x,y)) for x,y in zip(proposed_labels[which_wrong], correct_labels[which_wrong])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script 'conv_MNIST_torch.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
