{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear [MNIST](https://github.com/pytorch/examples/tree/master/mnist) with [torch](https://pytorch.org/)\n",
    "stough 202-\n",
    "\n",
    "Simple torch model with a single [Linear](https://pytorch.org/docs/stable/nn.html#linear)\n",
    "(or fully connected) layer \n",
    "from MNIST images to digit classification. This is simplified from \n",
    "the example just to see if torch learns a pretty simple per pixel\n",
    "prior.\n",
    "\n",
    "A [fully-connected layer](https://pytorch.org/docs/stable/nn.html#linear) with N inputs and M outputs\n",
    "solves a simple linear equation: $y = x A^T + b$, with a total of NxM + M parameters to learn.\n",
    "\n",
    "Additional resources:\n",
    "- A nice and recent [walkthrough](https://nextjournal.com/gkoehler/pytorch-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# or widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import copy\n",
    "import tempfile\n",
    "\n",
    "from functools import partial\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# from keras.datasets import mnist\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Module\n",
    "\n",
    "# For timing.\n",
    "import time\n",
    "tic, toc = (time.time, time.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Network\n",
    "The simplest possible network, simple linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x) # __call__\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    print('\\nTrain set: Average loss: {:.4f}'.format(loss_sum/len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Namespace to replace the argparse business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "# What is a Namespace? It looks like this. \n",
    "# Such objects allow you to use the dot operator, like a struct in C.\n",
    "# class Namespace:\n",
    "#     def __init__(self, **kwargs):\n",
    "#         self.__dict__.update(kwargs)\n",
    "\n",
    "args = Namespace(\n",
    "    no_cuda=False, \n",
    "    seed=1, \n",
    "    batch_size=64,\n",
    "    test_batch_size=1000,\n",
    "    epochs=5,\n",
    "    lr=1.0,\n",
    "    gamma=0.7,\n",
    "    log_interval=250,\n",
    "    save_model=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize the MNIST data\n",
    "\n",
    "The initial [Dataset](https://pytorch.org/docs/stable/data.html#) object returned implements `__getitem__()` and `__len__()`, and that's it pretty much. Below I collect this data into numpy arrays and use a custom class that inherits from Dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you: https://www.aiworkbox.com/lessons/load-mnist-dataset-from-pytorch-torchvision\n",
    "# https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "mnist_trainset = datasets.MNIST(root='/home/dip365/data', train=True, download=False, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='/home/dip365/data', train=False, download=False, transform=None)\n",
    "\n",
    "x_train = np.stack([np.array(x).ravel() for x,_ in mnist_trainset])\n",
    "x_test = np.stack([np.array(x).ravel() for x,_ in mnist_testset])\n",
    "y_train = np.stack([y for _,y in mnist_trainset]).astype('long')\n",
    "y_test = np.stack([y for _,y in mnist_testset]).astype('long')\n",
    "\n",
    "# These are Nx784\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_data(Dataset):\n",
    "    def __init__(self, XY):\n",
    "        self.x_data = XY[0]\n",
    "        self.y_data = XY[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x_data[idx], self.y_data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is a list of tuples (one for each input-output pair). \n",
    "# output of this function needs to be a single tuple of (inputs, outputs).\n",
    "def torch_collate(data):\n",
    "    inputs = np.stack([lyst[0] for lyst in data])\n",
    "    outputs = np.stack([lyst[1] for lyst in data])\n",
    "    return torch.from_numpy(inputs), torch.from_numpy(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Make torch DataLoaders for our custom dataset.\n",
    "After the below code executes, `train_loader` and `test_loader` are torch [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) objects that we iterate through to get batches of (N,784) data and (N,) target outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 4, 'pin_memory': False} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MNIST_data((x_train, y_train)),\n",
    "    batch_size=args.batch_size, shuffle=True, collate_fn=torch_collate, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MNIST_data((x_test, y_test)),\n",
    "    batch_size=args.test_batch_size, shuffle=True, collate_fn=torch_collate, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in test_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Instantiate and train the torch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "# Count the number of parameters: \n",
    "print(f'model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "if args.save_model:\n",
    "    torch.save(model.state_dict(), \"mnist_linear.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## View the weights that were learned.\n",
    "Recall that we learned a single fully connected layer, so something like 784x10 (plus 10 for bias). Let's see what these per-pixel weightings look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allweights = model.fc.weight.data.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allweights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2,5, figsize=(8,3))\n",
    "for i in range(5):\n",
    "    ax[0, i].imshow(np.reshape(allweights[i,:], (28,28)), cmap='bwr')\n",
    "    ax[1, i].imshow(np.reshape(allweights[i+5,:], (28,28)), cmap='bwr')\n",
    "\n",
    "[a.axes.get_xaxis().set_visible(False) for a in ax.flatten()]\n",
    "[a.axes.get_yaxis().set_visible(False) for a in ax.flatten()]\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
