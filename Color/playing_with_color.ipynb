{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Color\n",
    "stough 202-\n",
    "\n",
    "In this activity we'll use what we've learned about color to try and **enhance** an image or even **compress** it without too much perceptual loss. The other notebooks in this folder should be useful here, particularly [color_intro](./color_intro.ipynb), [color_HSV](./color_HSV.ipynb), and [color_YCbCr](./color_YCbCr.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "You can borrow the imports from any of the other notebooks in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhancement: Improving the Color of an Image\n",
    "Take a look at  `../dip_pics/sf.jpg` and try the `vis_hists` function on it to the distributions of the R,G,B channels. Of course you can try this out with any image you like of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To me it looks like the clouds shrouding the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge) are too dark, not blue enough really for my tastes. You can see this in the relative color distributions as well, with the blue component maxing out below the midpoint of the intensity range. \n",
    "\n",
    "Try adding some blue to the image to really make those clouds pop better. **Be careful** to use the `arr_info` to understand both the `dtype` and understood range of your data. (In fact, now might be a good time to review the [matplotlib_tutorial](../NumpyAndVisualization/matplotlib_tutorial.ipynb) on the kind of data [`imshow`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) and our [`vis_image`](../dip_utils/vis_utils.py).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ehancement: Improving Contrast through Saturation of Color\n",
    "Take a look at `../dip_pics/world_overexposed.jpg`. Here we have a very overexposed image. (Again, find any overexposed image you like). What this means generally is that we have a lot of what should be color/Hue showing up as mostly white; as in, it's been mixed with too much white. See the [color_HSV](./color_HSV.ipynb) demo titled \"Viewing the planes of the HSV space\" and see what happens as saturation $S_{HSV}$ approaches zero. \n",
    "\n",
    "Try converting the image to HSV and then scaling the saturation component to improve the color purity of all those washed-out pixels. Then convert back to RGB and see if it's improved. **Remember**, from the [skimage documentation](https://scikit-image.org/docs/dev/api/skimage.color.html#skimage.color.rgb2hsv), that the hsv channels are all in $[0,1]$, so when you do the scaling you might need to [`np.clip`]https://numpy.org/doc/stable/reference/generated/numpy.clip.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compression: Reducing the Bit-Depth of an Image to Save Space\n",
    "In a prior activity we reduced the bit-depth of the RGB color channels of an image and saw how that *quantization* affected the image quality, creating sharp edges of color change in what would be smoothly graded areas. So one of the limits of potentially compressing images relates to the minimum bit-depth that can maintain perceptual fidelity.\n",
    "\n",
    "But in this module's materials, we've seen that some color spaces (HSV, YCbCr, and L*a*b*) include some dimension that captures the brightness/luminance/intensity representation of an image separately from the \"color\" aspect. [YCbCr](./color_YCbCr.ipynb) in particular is used a lot in compression for precisely this reason.\n",
    "\n",
    "In the below cells, try considering an image like `../dip_pics/bellagio.jpg` or `../dip_pics/mountainSpring.jpg` for example. \n",
    "\n",
    "- See what happens when you reduce the bit-depth to 4-4-4 in RGB. \n",
    "- Then try reducing the bit-depth to 6-3-3 but **in the YCbCr** space. Here we're saving the same number of bits per pixel, but differentially quantizing (and therefore compressing) the **Luminance** and color components.\n",
    "\n",
    "It may be useful here to define a function that quantizes a 2D `ndarray` instead of a whole image at once. Luckily, it's likely the same quantization function you've already written may suffice. I include a useable version here just to help.  \n",
    "\n",
    "**Be careful**: [`skimage.color.ycbcr2rgb`](https://scikit-image.org/docs/dev/api/skimage.color.html#skimage.color.ycbcr2rgb) expects 'float64' data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_bitdepth(C, bitdepth=8):\n",
    "    \"\"\"\n",
    "    quantize_bitdepth(C, bitdepth=8): return a copy of the array C where the elements are\n",
    "    limited to 2**bitdepth unique values. Tries to handle a float image just in case.\n",
    "    \"\"\"\n",
    "    assert bitdepth <= 8, f'quantize_bitdepth error: expects bitdepth <= 8, got {bitdepth}'\n",
    "    \n",
    "    shft_amt = 8 - bitdepth\n",
    "    \n",
    "    J = np.uint8(C)\n",
    "    if C.dtype == 'float':\n",
    "        J = np.uint8(256 * (C/C.max()))\n",
    "        \n",
    "    return np.uint8((J >> shft_amt)*(255/(2**bitdepth-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
